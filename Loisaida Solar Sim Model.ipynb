{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nyisotoolkit\n",
    "from nyisotoolkit import NYISOData, NYISOStat,NYISOVis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error,r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from meteostat import Hourly, Point\n",
    "from datetime import datetime\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the NYISO API and got load and marginal cost data from the years 2022 and 2023. Then, I did the same using Meteostats' API. I found it to be easier to clean the data in R and then import back to Python. As I'm not sure how to use both languages together in the same notebook, I may release the work that I did in R separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Khalil/Downloads/merged_data.csv\")\n",
    "\n",
    "#Change column type to datetime64[ns] for column: 'date_time'\n",
    "df = df.astype({'date_time': 'datetime64[ns]'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm assuming that a precipitation value of \"NA\" accounts for days where there was no precipitation.\n",
    "df[\"prcp\"] = df[\"prcp\"].fillna(0)\n",
    "\n",
    "#I will also assume that the wind directions that are missing are southbound winds from the north, especially\n",
    "#since such rows always seem to have a wind speed value\n",
    "df[\"wdir\"] = df[\"wdir\"].fillna(0)\n",
    "\n",
    "\n",
    "#Drop the sun duration column because it's completely empty\n",
    "df.drop(columns=\"tsun\")\n",
    "\n",
    "# Change column type to category for column: 'coco'\n",
    "df = df.astype({'coco': 'category'})\n",
    "\n",
    "# Rename column 'LBMP....MWHr..N.Y.C.' to 'Price(MWhr)'\n",
    "df = df.rename(columns={'LBMP....MWHr..N.Y.C.': 'Price(MWhr)'})\n",
    "\n",
    "# Rename column 'Marginal.Cost.Losses....MWHr..N.Y.C.' to 'Marginal_Cost_Losses(MWHr)'\n",
    "df = df.rename(columns={'Marginal.Cost.Losses....MWHr..N.Y.C.': 'Marginal_Cost_Losses(MWHr)'})\n",
    "\n",
    "# Rename column 'Marginal.Cost.Congestion....MWHr..N.Y.C.' to 'Marginal_Cost_Congestion(MWHr)'\n",
    "df = df.rename(columns={'Marginal.Cost.Congestion....MWHr..N.Y.C.': 'Marginal_Cost_Congestion(MWHr)'})\n",
    "\n",
    "# Rename column 'N.Y.C.' to 'Load'\n",
    "df = df.rename(columns={'N.Y.C.': 'Load'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the weather condition code, less than 1% of the rows have missing values. The dictionary that the dataset\n",
    "came with explains that some weather stations either don't report weather conditions unless they are significant\n",
    "or they just don't report them at all. Ordinarily, I would use something like missForest for imputation or some other more sophisticated method but that's overkill for such a small proportion of missing data. It would be better to just use the mode of the column instead of just deleting them to retain as much data as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['coco'] = imputer.fit_transform(df[['coco']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was a little concerned about the skewness present in the columns, so I made these code blocks to transform them using the Yeo-Johnson transformation. But I was also concerned with how doing this may affect interpretability afterwards. I ended up leaving the data as is when making the models but I left these code blocks here in case I ever change my mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness\n",
    "print(df[\"LBMP....MWHr..N.Y.C.\"].skew())\n",
    "print(df[\"Marginal_Cost_Congestion(MWHr)\"].skew())\n",
    "print(df[\"Marginal.Cost.Losses....MWHr..N.Y.C.\"].skew())\n",
    "print(df[\"Load\"].skew())\n",
    "print(df[\"temp\"].skew())\n",
    "print(df[\"dwpt\"].skew())\n",
    "print(df[\"rhum\"].skew())\n",
    "print(df[\"prcp\"].skew())\n",
    "print(df[\"pres\"].skew())\n",
    "print(df[\"wdir\"].skew())\n",
    "\n",
    "\n",
    "#I tried to use sqrt, log, and BoxCox to transform the df but it either didn't work or yielded \n",
    "#unsatisfactory results. I'll use the Yeo-Johnson transformation instead to get a balance between sqrt and log\n",
    "#while not needing all of the df to be positive like BoxCox\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "df[\"LBMP....MWHr..N.Y.C.\"] = pt.fit_transform(df[['LBMP....MWHr..N.Y.C.']])\n",
    "df[\"Marginal_Cost_Congestion(MWHr)\"] = pt.fit_transform(df[['Marginal_Cost_Congestion(MWHr)]])\n",
    "df[\"Marginal_Cost_Losses(MWHr)\"]= pt.fit_transform(df[['Marginal_Cost_Losses(MWHr)]])\n",
    "df[\"Load\"]= pt.fit_transform(df[['Load']])\n",
    "df[\"temp\"]= pt.fit_transform(df[['temp']])\n",
    "df[\"dwpt\"]= pt.fit_transform(df[['dwpt']])\n",
    "df[\"rhum\"]= pt.fit_transform(df[['rhum']])\n",
    "df[\"prcp\"]= pt.fit_transform(df[['prcp']])\n",
    "df[\"pres\"]= pt.fit_transform(df[['pres']])\n",
    "df[\"wdir\"]= pt.fit_transform(df[['wdir']])\n",
    "df[\"coco\"]= pt.fit_transform(df[['coco']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of these columns are still skewed\n",
    "print(df[\"LBMP....MWHr..N.Y.C.\"].skew())\n",
    "print(df[\"Marginal_Cost_Congestion(MWHr)\"].skew())\n",
    "print(df[\"Marginal_Cost_Losses(MWHr)\"].skew())\n",
    "print(df[\"Load\"].skew())\n",
    "print(df[\"temp\"].skew())\n",
    "print(df[\"dwpt\"].skew())\n",
    "print(df[\"rhum\"].skew())\n",
    "print(df[\"prcp\"].skew())\n",
    "print(df[\"pres\"].skew())\n",
    "print(df[\"wdir\"].skew())\n",
    "print(df[\"coco\"].skew())\n",
    "\n",
    "df[\"Marginal_Cost_Congestion(MWHr)\"] = np.sqrt(df['Marginal_Cost_Congestion(MWHr)'])\n",
    "df[\"wdir\"] = np.sqrt(df['wdir'])\n",
    "df[\"prcp\"] = np.sqrt(df['prcp'])\n",
    "\n",
    "print(\"New skewness after sqrt transformation: \\n\")\n",
    "print(df[\"Marginal_Cost_Congestion(MWHr)\"].skew())\n",
    "print(df['wdir'].skew())\n",
    "print(df['prcp'].skew())\n",
    "\n",
    "#Those three columns were very stubborn in terms of their skewness \n",
    "# but at the very least I reduced it from what it was orginally by a significant margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract components from the date_time column\n",
    "df['year'] = df['date_time'].dt.year\n",
    "df['month'] = df['date_time'].dt.month\n",
    "df['day'] = df['date_time'].dt.day\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df['minute'] = df['date_time'].dt.minute\n",
    "df['second'] = df['date_time'].dt.second\n",
    "df['day_of_week'] = df['date_time'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['day_of_year'] = df['date_time'].dt.dayofyear\n",
    "df['week_of_year'] = df['date_time'].dt.isocalendar().week\n",
    "\n",
    "# Display the DataFrame with new features\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two years worth of data, let's use one to train our model(s) and the other to test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define the split date\n",
    "split_date = '2023-01-01 05:00:00'\n",
    "\n",
    "\n",
    "# Split the DataFrame into training and testing sets based on the split date\n",
    "train_data = df[df['date_time'] < split_date]\n",
    "test_data = df[df['date_time'] >= split_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Separate the features and response for training and testing sets\n",
    "response_var = \"Price(MWhr)\"\n",
    "\n",
    "X_train = train_data.drop(columns=[response_var,\"date_time\"])\n",
    "y_train = train_data[response_var]\n",
    "X_test = test_data.drop(columns=[response_var,\"date_time\"])\n",
    "y_test = test_data[response_var]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Testing set size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Creation/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "rf_rmse = root_mean_squared_error(y_test, rf_pred)\n",
    "print(f'Random Forest RMSE: {rf_rmse}')\n",
    "\n",
    "r_squared_forest = r2_score(y_test, rf_pred)\n",
    "print(f'Random Forest R-squared: {r_squared_forest}')\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(y_test.index, y_test, label='Actual')\n",
    "plt.plot(y_test.index, rf_pred, label='RF Predictions')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
